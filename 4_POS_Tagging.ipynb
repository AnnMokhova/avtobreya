{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "4_POS_Tagging.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcfCvtd-C7QU",
        "colab_type": "text"
      },
      "source": [
        "# POS-Tagging\n",
        "На этом семинаре мы рассмотрим некоторые библиотеки, позволяющие делать морфологический анализ, и даже обучим свой теггер на основе [BERT](https://habr.com/ru/post/436878/) (stay tuned!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT4u9VljC7Qb",
        "colab_type": "text"
      },
      "source": [
        "# NLTK\n",
        "Разнообразные теггеры находятся в модуле `nltk.tag`. Можно скачивать дополнительные модели (вспомним, как это делается?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtHmrHjWC7Qe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e7710d7-3d03-4a48-a166-3af44fd090a8"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/e3/389c2dd8d0e6ca1d8fad11aa4940e8df6909a26a5d954c0eff01f0d78b57/flair-0.4.3-py3-none-any.whl (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Collecting regex (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n",
            "Collecting langdetect (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.9MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.5)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Collecting segtok>=1.5.7 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Collecting bpemb>=0.2.9 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Collecting deprecated>=1.2.4 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/88/0e/9d5a1a8cd7130c49334cce7b8167ceda63d6a329c8ea65b626116bc9e9e6/Deprecated-1.2.6-py2.py3-none-any.whl\n",
            "Collecting ipython==7.6.1 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.0)\n",
            "Collecting pytorch-transformers>=1.1.0 (from flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 53.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting sqlitedict>=1.6.0 (from flair)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.16.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n",
            "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.2)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from ipython==7.6.1->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers>=1.1.0->flair) (1.9.236)\n",
            "Collecting sacremoses (from pytorch-transformers>=1.1.0->flair)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.9.11)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.236 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair) (1.12.236)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers>=1.1.0->flair) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->flair) (7.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.236->boto3->pytorch-transformers>=1.1.0->flair) (0.15.2)\n",
            "Building wheels for collected packages: regex, langdetect, mpld3, segtok, sqlitedict, sacremoses\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609250 sha256=cd0213f45270613f2cf2f548b55fc95be2b72f4b277e65401c8c693b9db74a7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=781a908b55b0c383c26780a418e5fb7691ec5338fee18e6dba66d16c1e5f4a1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=15643c4b607e87574744097ea689d7bf12c78268dad7137cf1806dcdc62035f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=92b4fdc00a7e09ddb202b193b4397ef6e88167f5ccfbf71e82c3b601dd01a25b\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=bd5c14d6dbb685899224974fc577e1793fa39dce304ed77e435f34f384e645ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=3810d8fd60dc83bc74794c607645ee5da1cce04f345b2b7527639b10e0a11308\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built regex langdetect mpld3 segtok sqlitedict sacremoses\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: regex, langdetect, mpld3, segtok, sentencepiece, bpemb, deprecated, prompt-toolkit, ipython, sacremoses, pytorch-transformers, sqlitedict, flair\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.6 flair-0.4.3 ipython-7.6.1 langdetect-1.0.7 mpld3-0.3 prompt-toolkit-2.0.10 pytorch-transformers-1.2.0 regex-2019.8.19 sacremoses-0.0.35 segtok-1.5.7 sentencepiece-0.1.83 sqlitedict-1.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS0G_LsOC7Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTsQoitC7Q2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f2ef0d10-9a37-4d42-b593-67548496aabe"
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YcEDsgBC7RI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "872a640b-e9de-48a4-dad5-35da15f67da8"
      },
      "source": [
        "brown.tagged_sents()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fp3INEjC7RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# корпуса делятся на части - categories\n",
        "print(brown.categories())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGOls1vVC7Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk.tag\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT6o49Y_HVuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e1d7d3c-57ac-4786-e62e-d85a347ded0c"
      },
      "source": [
        "brown.tagged_words()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzTyoCF3C7Rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54d93b16-c609-484e-e3ad-a93881e7a900"
      },
      "source": [
        "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
        "nltk.FreqDist(tags).max()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmBoQY2vC7Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "cd2fbc86-e4ad-4320-b380-0c8a3774603c"
      },
      "source": [
        "default_tagger = nltk.tag.DefaultTagger('NN')\n",
        "sentence = brown.sents()[5]\n",
        "print(sentence)\n",
        "default_tagger.tag(sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It', 'recommended', 'that', 'Fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'NN'),\n",
              " ('recommended', 'NN'),\n",
              " ('that', 'NN'),\n",
              " ('Fulton', 'NN'),\n",
              " ('legislators', 'NN'),\n",
              " ('act', 'NN'),\n",
              " ('``', 'NN'),\n",
              " ('to', 'NN'),\n",
              " ('have', 'NN'),\n",
              " ('these', 'NN'),\n",
              " ('laws', 'NN'),\n",
              " ('studied', 'NN'),\n",
              " ('and', 'NN'),\n",
              " ('revised', 'NN'),\n",
              " ('to', 'NN'),\n",
              " ('the', 'NN'),\n",
              " ('end', 'NN'),\n",
              " ('of', 'NN'),\n",
              " ('modernizing', 'NN'),\n",
              " ('and', 'NN'),\n",
              " ('improving', 'NN'),\n",
              " ('them', 'NN'),\n",
              " (\"''\", 'NN'),\n",
              " ('.', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDmu1kPUC7R8",
        "colab_type": "text"
      },
      "source": [
        "Не очень здорово, верно? Давайте хранить самые частые слова и приписывать им их самый частый тег (а остальным пока приписывать `NN`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y88X0mqwC7SB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "baa80480-ba49-4e69-9066-06a2b7fc7731"
      },
      "source": [
        "fd = nltk.FreqDist(brown.words(categories='news'))\n",
        "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
        "most_freq_words = fd.most_common(1000)\n",
        "likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
        "unigram_tagger = nltk.UnigramTagger(model=likely_tags, backoff=default_tagger)\n",
        "unigram_tagger.tag(sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It', 'PPS'),\n",
              " ('recommended', 'NN'),\n",
              " ('that', 'CS'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('legislators', 'NN'),\n",
              " ('act', 'NN'),\n",
              " ('``', '``'),\n",
              " ('to', 'TO'),\n",
              " ('have', 'HV'),\n",
              " ('these', 'DTS'),\n",
              " ('laws', 'NNS'),\n",
              " ('studied', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('revised', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'AT'),\n",
              " ('end', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('modernizing', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('improving', 'NN'),\n",
              " ('them', 'PPO'),\n",
              " (\"''\", \"''\"),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRwFUXpeC7SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d52b8b32-8006-4281-e522-a873fcf5fc83"
      },
      "source": [
        "# Можно легко оценить на каких-нибудь предложениях из того же корпуса\n",
        "unigram_tagger.evaluate(brown.tagged_sents(categories='reviews'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68452731918239"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWzuv8NC7Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Можно просто приписывать самый частый тег всем словам из корпуса\n",
        "unigram_tagger = nltk.UnigramTagger(brown.tagged_sents(categories='news'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N3Hxse9C7Sm",
        "colab_type": "text"
      },
      "source": [
        "## Задание\n",
        "Используя [документацию](http://www.nltk.org/api/nltk.tag.html#nltk.tag.sequential.BigramTagger) и функцию `help()` обучите биграммный теггер. Сравните его с униграммным на примерах, где должно стать лучше (выбор тега зависит от контекста), например: `They wind back the clock.` vs. `The wind rises.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp1RFNdQC7Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bigram_tagger = nltk.tag.sequential.BigramTagger(train = brown.tagged_sents(), \n",
        "#                                                 model=likely_tags, \n",
        "#                                                 backoff = default_tagger)\n",
        "\n",
        "train = list(brown.tagged_sents())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcY_w4juC7S2",
        "colab_type": "text"
      },
      "source": [
        "Помимо этого в NLTK реализовано множество теггеров:\n",
        "* regexp tagger\n",
        "* ngram tagger\n",
        "* Brill tagger\n",
        "* CRF tagger\n",
        "* etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuaPQkNSC7S7",
        "colab_type": "text"
      },
      "source": [
        "## spaCy\n",
        "[SpaCy](https://spacy.io/) — библиотека для обработки текстов, содержащая разные модули (токенайзер, pos-tagging, NER etc.)\n",
        "Разработана специально для построения пайплайнов, сейчас нас будет интересовать токенизация и морфологический анализ.\n",
        "Недостаток — некоторые модели (например, для NER) нужно загружать отдельно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uqdDS8TC7S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp9WjaIuC7TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hylMLqw7C7TP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "30e4b098-55f5-41b9-a58e-5ee9ae9dc772"
      },
      "source": [
        "nltk.download('webtext')\n",
        "from nltk.corpus import webtext"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJei8vr5C7TW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_text_id = webtext.fileids()[0]\n",
        "sentences = webtext.raw(first_text_id).split('\\r\\n')\n",
        "text = \" \".join(sentences[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_gz6U7_C7Tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "15112f60-3cb0-43b1-ca83-3b3c5144b37e"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay checked When in full screen mode Pressing Ctrl-N should open a new browser when only download dialog is left open add icons to context menu So called \"tab bar\" should be made a proper toolbar or given the ability collapse / expand. [XUL] Implement Cocoa-style toolbar customization. #ifdefs for MOZ_PHOENIX customize dialog's toolbar has small icons when small icons is not checked nightly builds and tinderboxen for Phoenix finish tearing prefs UI to pieces and then make it not suck\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nECohggvC7Tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43d564fa-4ee9-4998-b59a-311df4c50e30"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Загружаем весь пайплайн для английского\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Обрабатываем текст\n",
        "doc = nlp(text)\n",
        "\n",
        "# Выведем токены, леммы и теги\n",
        "for i, s in enumerate(doc.sents):\n",
        "    print(\"\\n-- Sentence %d --\" % i)\n",
        "    for t in s:\n",
        "        print(t.text, t.lemma_, t.pos_, sep=\"\\t\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-- Sentence 0 --\n",
            "Cookie\tCookie\tPROPN\n",
            "Manager\tManager\tPROPN\n",
            ":\t:\tPUNCT\n",
            "\"\t\"\tPUNCT\n",
            "Do\tdo\tVERB\n",
            "n't\tnot\tADV\n",
            "allow\tallow\tVERB\n",
            "sites\tsite\tNOUN\n",
            "that\tthat\tDET\n",
            "set\tset\tVERB\n",
            "removed\tremove\tVERB\n",
            "cookies\tcookie\tNOUN\n",
            "to\tto\tPART\n",
            "set\tset\tVERB\n",
            "future\tfuture\tADJ\n",
            "cookies\tcookie\tNOUN\n",
            "\"\t\"\tPUNCT\n",
            "should\tshould\tVERB\n",
            "stay\tstay\tVERB\n",
            "checked\tcheck\tVERB\n",
            "\n",
            "-- Sentence 1 --\n",
            "When\twhen\tADV\n",
            "in\tin\tADP\n",
            "full\tfull\tADJ\n",
            "screen\tscreen\tNOUN\n",
            "mode\tmode\tNOUN\n",
            "\n",
            "-- Sentence 2 --\n",
            "Pressing\tpress\tVERB\n",
            "Ctrl\tCtrl\tPROPN\n",
            "-\t-\tPUNCT\n",
            "N\tN\tPROPN\n",
            "should\tshould\tVERB\n",
            "open\topen\tVERB\n",
            "a\ta\tDET\n",
            "new\tnew\tADJ\n",
            "browser\tbrowser\tNOUN\n",
            "when\twhen\tADV\n",
            "only\tonly\tADJ\n",
            "download\tdownload\tNOUN\n",
            "dialog\tdialog\tNOUN\n",
            "is\tbe\tVERB\n",
            "left\tleave\tVERB\n",
            "open\topen\tADJ\n",
            "add\tadd\tNOUN\n",
            "icons\ticon\tNOUN\n",
            "to\tto\tADP\n",
            "context\tcontext\tNOUN\n",
            "menu\tmenu\tNOUN\n",
            "\n",
            "-- Sentence 3 --\n",
            "So\tso\tADV\n",
            "called\tcall\tVERB\n",
            "\"\t\"\tPUNCT\n",
            "tab\ttab\tNOUN\n",
            "bar\tbar\tNOUN\n",
            "\n",
            "-- Sentence 4 --\n",
            "\"\t\"\tPUNCT\n",
            "should\tshould\tVERB\n",
            "be\tbe\tVERB\n",
            "made\tmake\tVERB\n",
            "a\ta\tDET\n",
            "proper\tproper\tADJ\n",
            "toolbar\ttoolbar\tNOUN\n",
            "or\tor\tCCONJ\n",
            "given\tgive\tVERB\n",
            "the\tthe\tDET\n",
            "ability\tability\tNOUN\n",
            "collapse\tcollapse\tNOUN\n",
            "/\t/\tSYM\n",
            "expand\texpand\tNOUN\n",
            ".\t.\tPUNCT\n",
            "\n",
            "-- Sentence 5 --\n",
            "[\t[\tPUNCT\n",
            "XUL\tXUL\tPROPN\n",
            "]\t]\tPUNCT\n",
            "Implement\tImplement\tPROPN\n",
            "Cocoa\tCocoa\tPROPN\n",
            "-\t-\tPUNCT\n",
            "style\tstyle\tNOUN\n",
            "toolbar\ttoolbar\tNOUN\n",
            "customization\tcustomization\tNOUN\n",
            ".\t.\tPUNCT\n",
            "\n",
            "-- Sentence 6 --\n",
            "#\t#\tSYM\n",
            "ifdefs\tifdef\tNOUN\n",
            "for\tfor\tADP\n",
            "MOZ_PHOENIX\tmoz_phoenix\tNOUN\n",
            "customize\tcustomize\tNOUN\n",
            "dialog\tdialog\tNOUN\n",
            "'s\t's\tPART\n",
            "toolbar\ttoolbar\tNOUN\n",
            "has\thave\tVERB\n",
            "small\tsmall\tADJ\n",
            "icons\ticon\tNOUN\n",
            "when\twhen\tADV\n",
            "small\tsmall\tADJ\n",
            "icons\ticon\tNOUN\n",
            "is\tbe\tVERB\n",
            "not\tnot\tADV\n",
            "checked\tcheck\tVERB\n",
            "nightly\tnightly\tADV\n",
            "builds\tbuild\tNOUN\n",
            "and\tand\tCCONJ\n",
            "tinderboxen\ttinderboxen\tNOUN\n",
            "for\tfor\tADP\n",
            "Phoenix\tPhoenix\tPROPN\n",
            "finish\tfinish\tNOUN\n",
            "tearing\ttear\tVERB\n",
            "prefs\tpref\tNOUN\n",
            "UI\tUI\tPROPN\n",
            "to\tto\tADP\n",
            "pieces\tpiece\tNOUN\n",
            "and\tand\tCCONJ\n",
            "then\tthen\tADV\n",
            "make\tmake\tVERB\n",
            "it\t-PRON-\tPRON\n",
            "not\tnot\tADV\n",
            "suck\tsuck\tVERB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_6RbOAvC7T5",
        "colab_type": "text"
      },
      "source": [
        "## Самостоятельно:\n",
        "возьмите любой текст на английском языке (например, скопируйте абзац из Википедии), примените к нему пайплайн spaCy, \n",
        "выведите все биграммы, состоящие из прилагательного и существительного.\n",
        "\n",
        "**Бонус-трек**: перепишите ваш код в виде функции, которая принимает на вход текст и последовательность тегов, а возвращает энграммы, соответствующие данной последовательности. Протестируйте функцию на 3/4-граммах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmkZO1Z3C7UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"\"\"\n",
        "\"\"\"\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV8cSOXiC7US",
        "colab_type": "text"
      },
      "source": [
        "# Flair\n",
        "[Flair](https://github.com/zalandoresearch/flair) — библиотека для работы с векторными представлениями слов, содержит компоненты для решения \n",
        "разных задач NLP (sequence tagging, classification ...). Есть как готовые модели, так и возможность обучения на своих данных, дообучение предобученные векторных представлений и т.д.\n",
        "\n",
        "Сегодня рассмотрим некоторые возможности flair для морфологического анализа на примере англоязычных данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM_H31sHC7UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install flair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLUBH-1_C7Ub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "2ff0b622-394f-4439-a450-709cba38d2d8"
      },
      "source": [
        "# импортируем простейшие объекты\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# создаем предложение\n",
        "sentence = Sentence('Moscow is the capital of Russia .')\n",
        "\n",
        "# загружаем модель теггинга\n",
        "tagger = SequenceTagger.load('pos')\n",
        "\n",
        "# обрабатываем предложение\n",
        "# ВАЖНО: объект `sentence` при этом меняется\n",
        "tagger.predict(sentence)\n",
        "\n",
        "print(sentence)\n",
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:51:33,031 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/POS-ontonotes--h256-l1-b32-p3-0.5-%2Bglove%2Bnews-forward%2Bnews-backward-normal-locked0.5-word0.05--v0.4_0/en-pos-ontonotes-v0.4.pt not found in cache, downloading to /tmp/tmpzt_9czau\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 432218302/432218302 [00:19<00:00, 22517323.62B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:51:52,745 copying /tmp/tmpzt_9czau to cache at /root/.flair/models/en-pos-ontonotes-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:51:53,846 removing temp file /tmp/tmpzt_9czau\n",
            "2019-10-04 14:51:53,907 loading file /root/.flair/models/en-pos-ontonotes-v0.4.pt\n",
            "Sentence: \"Moscow is the capital of Russia .\" - 7 Tokens\n",
            "Moscow <PROPN> is <VERB> the <DET> capital <NOUN> of <ADP> Russia <PROPN> . <PUNCT>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAYeg9IAC7Ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3984ecc1-f8bd-46d6-c5be-f74028dd552b"
      },
      "source": [
        "# если интересно узнать подробнее об этих объектах\n",
        "help(sentence)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Sentence in module flair.data object:\n",
            "\n",
            "class Sentence(DataPoint)\n",
            " |  A Sentence is a list of Tokens and is used to represent a sentence or text fragment.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Sentence\n",
            " |      DataPoint\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __copy__(self)\n",
            " |  \n",
            " |  __getitem__(self, idx:int) -> flair.data.Token\n",
            " |  \n",
            " |  __init__(self, text:str=None, use_tokenizer:bool=False, labels:Union[List[flair.data.Label], List[str]]=None, language_code:str=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  add_label(self, label:Union[flair.data.Label, str])\n",
            " |  \n",
            " |  add_labels(self, labels:Union[List[flair.data.Label], List[str]])\n",
            " |  \n",
            " |  add_token(self, token:Union[flair.data.Token, str])\n",
            " |  \n",
            " |  clear_embeddings(self, embedding_names:List[str]=None)\n",
            " |  \n",
            " |  convert_tag_scheme(self, tag_type:str='ner', target_scheme:str='iob')\n",
            " |  \n",
            " |  get_embedding(self) -> <built-in method tensor of type object at 0x7efde342d620>\n",
            " |  \n",
            " |  get_label_names(self) -> List[str]\n",
            " |  \n",
            " |  get_language_code(self) -> str\n",
            " |  \n",
            " |  get_spans(self, tag_type:str, min_score=-1) -> List[flair.data.Span]\n",
            " |  \n",
            " |  get_token(self, token_id:int) -> flair.data.Token\n",
            " |  \n",
            " |  infer_space_after(self)\n",
            " |      Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n",
            " |      tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n",
            " |      :return:\n",
            " |  \n",
            " |  set_embedding(self, name:str, vector)\n",
            " |  \n",
            " |  to(self, device:str)\n",
            " |  \n",
            " |  to_dict(self, tag_type:str=None)\n",
            " |  \n",
            " |  to_original_text(self) -> str\n",
            " |  \n",
            " |  to_plain_string(self)\n",
            " |  \n",
            " |  to_tagged_string(self, main_tag=None) -> str\n",
            " |  \n",
            " |  to_tokenized_string(self) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  embedding\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DataPoint:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUflp3JC7Up",
        "colab_type": "text"
      },
      "source": [
        "У flair есть встроенные датасеты (`flair.datasets`), можно попробовать использовать их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW7HhRfzC7Ur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "95268c19-02eb-4a2a-f7ab-9bdb038e082c"
      },
      "source": [
        "import flair.datasets\n",
        "corpus = flair.datasets.UD_ENGLISH()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:20,885 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu not found in cache, downloading to /tmp/tmpz4esi45i\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1668174B [00:00, 37222916.73B/s]         "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:20,991 copying /tmp/tmpz4esi45i to cache at /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
            "2019-10-04 14:57:20,996 removing temp file /tmp/tmpz4esi45i\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:21,320 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu not found in cache, downloading to /tmp/tmp17q79_io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1661985B [00:00, 34881733.83B/s]         "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:21,430 copying /tmp/tmp17q79_io to cache at /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "2019-10-04 14:57:21,443 removing temp file /tmp/tmp17q79_io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:22,196 https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu not found in cache, downloading to /tmp/tmp_cxdr_7y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "13303045B [00:00, 78753281.36B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:22,437 copying /tmp/tmp_cxdr_7y to cache at /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-10-04 14:57:22,464 removing temp file /tmp/tmp_cxdr_7y\n",
            "2019-10-04 14:57:22,900 Reading data from /root/.flair/datasets/ud_english\n",
            "2019-10-04 14:57:22,901 Train: /root/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
            "2019-10-04 14:57:22,902 Test: /root/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
            "2019-10-04 14:57:22,903 Dev: /root/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHqCEmQlC7Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a893e81d-3806-4f5e-8e5b-d5df33819ef1"
      },
      "source": [
        "corpus.test[0].to_tagged_string('upos')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What <PRON> if <SCONJ> Google <PROPN> Morphed <VERB> Into <ADP> GoogleOS <PROPN> ? <PUNCT>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmXEpl7iC7U5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(corpus.test[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jglGeXZcC7VA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import choice\n",
        "n = choice(range(len(corpus.test)))\n",
        "sentence = corpus.test[n]\n",
        "print(sentence.to_tagged_string('upos'))\n",
        "print()\n",
        "\n",
        "# помним, что теперь `sentence` поменяется\n",
        "tagger.predict(sentence)\n",
        "\n",
        "print(sentence)\n",
        "print(sentence.to_tagged_string('upos'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6N0aOAKC7VG",
        "colab_type": "text"
      },
      "source": [
        "И наконец: **flair** позволяет обучать свои модели. Попробуем использовать эмбеддинги от Google, чтобы обучить свой теггер, вот [так](https://colab.research.google.com/drive/1OZN14wo1QGiwSpFhYuKPOPbzmJ2CzeWU)."
      ]
    }
  ]
}